{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: Prepare a dataset and train AutoEncoder model\n",
    "\n",
    "In this example, we will learn how to create a dataset containing acceleration data for training a simple AutoEncoder model for anomaly detection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and check data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of metadata: 4\n",
      "record_id: 016_00000000030000000215\n",
      "record_id: 016_00000000030000000240\n",
      "record_id: 253_16000000080000000747\n",
      "record_id: W07_17000000020000001255\n"
     ]
    }
   ],
   "source": [
    "from dwtk.db import V3DBHandler as DBHandler\n",
    "\n",
    "db_handler = DBHandler(\n",
    "    db_class='meta',\n",
    "    db_host='/data_pool_1/small_DrivingBehaviorDatabase/dwtk.db',\n",
    "    base_dir_path='/data_pool_1/small_DrivingBehaviorDatabase',\n",
    "    read_on_init=False\n",
    ")\n",
    "db_handler.read(where=\n",
    "                'tags like \"%acceleration%\" '\n",
    "                'and tags not like \"%gps%\" '\n",
    "                'and path not like \"%lidar%\"'\n",
    "               )\n",
    "\n",
    "print('# of metadata: {}'.format(len(db_handler)))\n",
    "for i, meta in enumerate(db_handler):\n",
    "    print(\"record_id: {}\".format(meta[\"record_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we only collect acceleration data of 4 trips as we just want to see the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load Python extension for LZ4 support. LZ4 compression will not be available.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf045fcdbfc64f86b33f968515152a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data dimemsion: 3\n"
     ]
    }
   ],
   "source": [
    "from dwtk.io import BaseFileReader\n",
    "from dwtk.preprocesses.downsampling import Downsample\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "reader = BaseFileReader()\n",
    "reader.add_preprocess(Downsample(target_frame_rate=1))\n",
    "\n",
    "dump_dir = \"dump\"\n",
    "data_dir = os.path.join(dump_dir, \"down\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "feats_scp = os.path.join(data_dir, \"feats.scp\")\n",
    "\n",
    "f = open(feats_scp, mode='w')\n",
    "for meta in tqdm(db_handler):\n",
    "    timestamps, data, columns = reader.read(meta)\n",
    "    feats_path = os.path.join(data_dir, meta[\"record_id\"] + \".npy\")\n",
    "    np.save(feats_path, data.astype(np.float32))\n",
    "    f.write(\"{0} {1}\\n\".format(meta[\"record_id\"], feats_path))\n",
    "\n",
    "f.close()\n",
    "\n",
    "dim = data.shape[1]\n",
    "print(\"Data dimemsion: {}\".format(dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train anomaly-detection model\n",
    "Install [nadt](https://github.com/TakedaLab/neural-anomaly-detection-toolkit) library to use anomaly detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/TakedaLab/neural-anomaly-detection-toolkit.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path setting\n",
    "exp_dir = \"exp\"\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "checkpoint_dir = os.path.join(exp_dir, \"checkpoint\")\n",
    "log_dir = os.path.join(exp_dir, \"lightning_logs\")\n",
    "checkpoint_file = os.path.join(checkpoint_dir, \"last.ckpt\")\n",
    "checkpoint_file = checkpoint_file if os.path.isfile(checkpoint_file) else None\n",
    "\n",
    "# Model setting\n",
    "algorithm = \"AutoEncoder\"\n",
    "algorithm_params = {\n",
    "    \"num_features\": dim,\n",
    "    \"sequence_length\": 128,\n",
    "    \"num_hidden_layers\": 4,\n",
    "    \"num_hidden_units\": 128,\n",
    "    \"num_latent_units\": 8,\n",
    "    \"dropout\": 0.1,\n",
    "}\n",
    "# Training setting\n",
    "optimizer = \"Adam\"\n",
    "optimizer_params = {\"lr\": 0.001}\n",
    "training_params = {\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 512,\n",
    "    \"saving_period\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nadt.datasets.dataset import DataSet\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "algorithm_dataset = DataSet(\n",
    "    feats_scp=feats_scp,\n",
    "    sequence_length=algorithm_params[\"sequence_length\"],\n",
    "    annotation=False,\n",
    ")\n",
    "algorithm_dataloader = DataLoader(\n",
    "    dataset=algorithm_dataset, shuffle=True, num_workers=4,\n",
    "    batch_size=training_params[\"batch_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name      | Type        | Params\n",
      "------------------------------------------\n",
      "0 | algorithm | AutoEncoder | 202 K \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120db73841fe42e39ec15341f9ff436b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nadt.algorithms.wrapper import Algorithm\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.logging import TensorBoardLogger\n",
    "\n",
    "# Set model\n",
    "algorithm_model = Algorithm(\n",
    "    algorithm=algorithm,\n",
    "    algorithm_params=algorithm_params,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_params=optimizer_params,\n",
    ")\n",
    "\n",
    "# Set trainer\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_last=True,\n",
    "    monitor=\"loss\",\n",
    "    filepath=checkpoint_dir + \"/{epoch}_{loss:.2f}\",\n",
    "    period=training_params[\"saving_period\"]\n",
    ")\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=log_dir,\n",
    "    version=\"latest\",\n",
    ")\n",
    "gpus = None\n",
    "algorithm_trainer = pl.Trainer(checkpoint_callback=checkpoint_callback,\n",
    "    logger=logger,\n",
    "    resume_from_checkpoint=checkpoint_file,\n",
    "    max_epochs=training_params[\"num_epochs\"],\n",
    "    gpus=gpus, auto_select_gpus=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "algorithm_trainer.fit(algorithm_model, algorithm_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}